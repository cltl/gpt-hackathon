+++
title = "References"
hascode = false
date = Date(2023, 4, 11)
+++


# References 

* Piantadowsky (2023): [Modern Language Models Refute Chomsky’s Approach to Language.](https://lingbuzz.net/lingbuzz/007180)
* Webson and Pavlick (2022): [Do Prompt-Based Models Really Understand the Meaning of Their Prompts?](https://aclanthology.org/2022.naacl-main.167.pdf)
* Webson et al. (2023): [Are Language Models Worse than Humans at Following Prompts? It's Complicated](https://arxiv.org/abs/2301.07085)
* Petroni et al. (2019): [Language Models as Knowledge Bases?](https://aclanthology.org/D19-1250.pdf)
* Schick and Schütze (NAACL 2021): [It's not just size that matters - small LMs are also few-shot learners](https://aclanthology.org/2021.naacl-main.185.pdf)
* Shin et al. (EMNLP 2020): [AutoPrompt: Eliciting knowledge from LMs with automatically generated prompts](https://aclanthology.org/2020.emnlp-main.346/)
* Li and Liang (2021): [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353.pdf)
* He et al. (2022): [Towards a Unified View on Visual Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=0RDcd5Axok)
* Liu et al. (2021): [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)
* Lu et al. (2022): [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://aclanthology.org/2022.acl-long.556.pdf)
* Gong, Shansan, et al.(2022): [DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/pdf/2210.08933.pdf)
* Leister et al. (2021): [The Power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf)



