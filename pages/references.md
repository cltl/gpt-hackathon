+++
title = "References"
hascode = false
date = Date(2023, 4, 11)
+++


# References 

## Prompting
* Piantadowsky (2023): [Modern Language Models Refute Chomsky’s Approach to Language.](https://lingbuzz.net/lingbuzz/007180)
* Webson and Pavlick (2022): [Do Prompt-Based Models Really Understand the Meaning of Their Prompts?](https://aclanthology.org/2022.naacl-main.167.pdf)
* Webson et al. (2023): [Are Language Models Worse than Humans at Following Prompts? It's Complicated](https://arxiv.org/abs/2301.07085)
* Petroni et al. (2019): [Language Models as Knowledge Bases?](https://aclanthology.org/D19-1250.pdf)
* Schick and Schütze (NAACL 2021): [It's not just size that matters - small LMs are also few-shot learners](https://aclanthology.org/2021.naacl-main.185.pdf)
* Shin et al. (EMNLP 2020): [AutoPrompt: Eliciting knowledge from LMs with automatically generated prompts](https://aclanthology.org/2020.emnlp-main.346/)
* Li and Liang (2021): [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353.pdf)
* He et al. (2022): [Towards a Unified View on Visual Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=0RDcd5Axok)
* Liu et al. (2021): [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)
* Lu et al. (2022): [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://aclanthology.org/2022.acl-long.556.pdf)
* Leister et al. (2021): [The Power of Scale for Parameter-Efficient Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf)
* Wang et al. (2022): [SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions](https://arxiv.org/pdf/2212.10560.pdf)
* Brown et al. (2020): [Language Models are few-shot learners](https://arxiv.org/abs/2005.14165)


## Sequence to sequence models / Conditional generation  
* Lewis et al. (2019): [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
* Liu et al. (2020): [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/pdf/2001.08210.pdf)
* Raffel et al. (2020): [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/volume21/20-074/20-074.pdf) 
* Xue et al. (2020): [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)
* Xue et al. (2021): [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
* Gong, Shansan, et al.(2022): [DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/pdf/2210.08933.pdf)


