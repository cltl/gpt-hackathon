<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/gpt-hackaton/css/franklin.css"> <link rel=stylesheet  href="/gpt-hackaton/css/jemdoc.css"> <link rel=icon  href="/gpt-hackaton/assets/favicon.png"> <title>References</title> <main class=outside > <div class=box > <aside class=layout-menu > <div class=menu-category >intro</div> <div class="menu-item "><a href="/gpt-hackaton/">Info and schedule</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/contributing/">Contributing</a></div> <div class=menu-category >topics</div> <div class="menu-item "><a href="/gpt-hackaton/pages/uncertainty_confidence/">Uncertainty and Confidence</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/framenet/">Framenet</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/historic_dutch/">Historic Dutch</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/medical_notes/">Medical notes</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/toxic_language/">Toxic language</a></div> <div class=menu-category >resources</div> <div class="menu-item current"><a href="/gpt-hackaton/pages/references/">References</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/tutorials/">Tutorials/Courses</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/running_llama/">Running LLMs Locally</a></div> <div class="menu-item "><a href="/gpt-hackaton/pages/tools/">Tools</a></div> </aside> <div class=layout-content > <div class=franklin-content > <h1 id=references ><a href="#references" class=header-anchor >References </a></h1> <h2 id=prompting ><a href="#prompting" class=header-anchor >Prompting</a></h2> <ul> <li><p>Piantadowsky &#40;2023&#41;: <a href="https://lingbuzz.net/lingbuzz/007180">Modern Language Models Refute Chomsky’s Approach to Language.</a></p> <li><p>Webson and Pavlick &#40;2022&#41;: <a href="https://aclanthology.org/2022.naacl-main.167.pdf">Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</a></p> <li><p>Webson et al. &#40;2023&#41;: <a href="https://arxiv.org/abs/2301.07085">Are Language Models Worse than Humans at Following Prompts? It&#39;s Complicated</a></p> <li><p>Petroni et al. &#40;2019&#41;: <a href="https://aclanthology.org/D19-1250.pdf">Language Models as Knowledge Bases?</a></p> <li><p>Schick and Schütze &#40;NAACL 2021&#41;: <a href="https://aclanthology.org/2021.naacl-main.185.pdf">It&#39;s not just size that matters - small LMs are also few-shot learners</a></p> <li><p>Shin et al. &#40;EMNLP 2020&#41;: <a href="https://aclanthology.org/2020.emnlp-main.346/">AutoPrompt: Eliciting knowledge from LMs with automatically generated prompts</a></p> <li><p>Li and Liang &#40;2021&#41;: <a href="https://aclanthology.org/2021.acl-long.353.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></p> <li><p>He et al. &#40;2022&#41;: <a href="https://openreview.net/forum?id&#61;0RDcd5Axok">Towards a Unified View on Visual Parameter-Efficient Transfer Learning</a></p> <li><p>Liu et al. &#40;2021&#41;: <a href="https://arxiv.org/pdf/2107.13586.pdf">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a></p> <li><p>Lu et al. &#40;2022&#41;: <a href="https://aclanthology.org/2022.acl-long.556.pdf">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a></p> <li><p>Leister et al. &#40;2021&#41;: <a href="https://aclanthology.org/2021.emnlp-main.243.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a></p> <li><p>Wang et al. &#40;2022&#41;: <a href="https://arxiv.org/pdf/2212.10560.pdf">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions</a></p> </ul> <h2 id=sequence_to_sequence_models_conditional_generation ><a href="#sequence_to_sequence_models_conditional_generation" class=header-anchor >Sequence to sequence models / Conditional generation </a></h2> <ul> <li><p>Lewis et al. &#40;2019&#41;: <a href="https://arxiv.org/pdf/1910.13461.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></p> <li><p>Liu et al. &#40;2020&#41;: <a href="https://arxiv.org/pdf/2001.08210.pdf">Multilingual Denoising Pre-training for Neural Machine Translation</a></p> <li><p>Raffel et al. &#40;2020&#41;: <a href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> </p> <li><p>Xue et al. &#40;2020&#41;: <a href="https://arxiv.org/abs/2010.11934">mT5: A massively multilingual pre-trained text-to-text transformer</a></p> <li><p>Xue et al. &#40;2021&#41;: <a href="https://arxiv.org/abs/2105.13626">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a></p> <li><p>Gong, Shansan, et al.&#40;2022&#41;: <a href="https://arxiv.org/pdf/2210.08933.pdf">DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</a></p> </ul> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> CLTL. Last modified: April 26, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </main>