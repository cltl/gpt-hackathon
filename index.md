@def title = "GPT Hackathon"

# Hallucination and bias: fight or exploit?

With the release of ChatGPT, the power of and debate around language models have reached the wider public discourse. Even though Large Language Models have been present within the field of NLP for a couple of years now, ChatGPT has raised questions about the role of LLMs in our research. 

LLMs produce highly fluent and natural-sounding text, which indicates that they have a good representation of many linguistic regularities. At the same time, it is a well-known tendency that they can make up facts and produce entirely nonsensical output with a high degree of confidence and express it in persuasive language (*hallucination*). In addition, LLMs are likely to reflect social biases present in their training data. 

In this hackathon, we want to examine the tension between a high degree of linguistic competence and the danger of hallucination and reproduction of biases. We propose to work on tasks and datasets related to several of our research projects. Some tasks are about understanding text and producing labels (*fight* hallucination and bias), while others focus on how we could harness language models to augment datasets (*exploit* hallucination and bias). 

